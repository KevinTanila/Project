{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base and ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import string\n",
    "import warnings\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from time import time\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For web-scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import feedparser as fp\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# The Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scraped_articles_true.json') as data_file:\n",
    "    dtrue = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, site in enumerate((list(dtrue['newspapers']))):\n",
    "    articles = list(dtrue['newspapers'][site]['articles'])\n",
    "    if i == 0:\n",
    "        X_true = pd.DataFrame.from_dict(articles)\n",
    "    else:\n",
    "        new_df = pd.DataFrame.from_dict(articles)\n",
    "        X_true = pd.concat([X_true, new_df], ignore_index = True, sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scraped_articles_false.json') as data_file:\n",
    "    dfalse = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, site in enumerate((list(dfalse['newspapers']))):\n",
    "    articles = list(dfalse['newspapers'][site]['articles'])\n",
    "    if i == 0:\n",
    "        X_false = pd.DataFrame.from_dict(articles)\n",
    "    else:\n",
    "        new_df = pd.DataFrame.from_dict(articles)\n",
    "        X_false = pd.concat([X_false, new_df], ignore_index = True, sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a label of 0 is real, a label of 1 is fake\n",
    "X_true['label'] = 1\n",
    "X_false['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true[\"author\"] = X_true[\"author\"].apply(lambda x: x[0] if len(x) > 0 else np.NaN)\n",
    "X_false[\"author\"] = X_false[\"author\"].apply(lambda x: x[0] if len(x) > 0 else np.NaN)\n",
    "\n",
    "X_true.dropna(axis = 0, inplace = True)\n",
    "X_false.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_true) > len(X_false):\n",
    "    X = pd.concat([X_true.sample(n = len(X_false)), X_false], ignore_index = True, sort = True)\n",
    "else:\n",
    "    X = pd.concat([X_true, X_false.sample(n = len(X_true))], ignore_index = True, sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "      <th>link</th>\n",
       "      <th>published</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Helen Sullivan</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.theguardian.com/world/2020/may/25/...</td>\n",
       "      <td>2020-05-25T00:00:00</td>\n",
       "      <td>Key developments in the global coronavirus out...</td>\n",
       "      <td>Coronavirus: at a glance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nicola Davis</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.theguardian.com/world/2020/may/24/...</td>\n",
       "      <td>2020-05-24T00:00:00</td>\n",
       "      <td>Explainer: what do we now know about Covid-19 ...</td>\n",
       "      <td>Explainer: what do we now know about Covid-19 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ian Sample</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.theguardian.com/world/2020/may/22/...</td>\n",
       "      <td>2020-05-22T00:00:00</td>\n",
       "      <td>Politicians have become more cautious about im...</td>\n",
       "      <td>Why we might not get a coronavirus vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sarah Boseley</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.theguardian.com/science/2020/may/2...</td>\n",
       "      <td>2020-05-22T00:00:00</td>\n",
       "      <td>Hydroxychloroquine, the anti-malarial drug Don...</td>\n",
       "      <td>Hydroxychloroquine: Trump's Covid-19 'cure' in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ed Aarons</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.theguardian.com/sport/2020/mar/13/...</td>\n",
       "      <td>2020-03-13T00:00:00</td>\n",
       "      <td>From major club and international football to ...</td>\n",
       "      <td>Coronavirus and sport – a list of the major ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author  label                                               link  \\\n",
       "0  Helen Sullivan      1  https://www.theguardian.com/world/2020/may/25/...   \n",
       "1    Nicola Davis      1  https://www.theguardian.com/world/2020/may/24/...   \n",
       "2      Ian Sample      1  https://www.theguardian.com/world/2020/may/22/...   \n",
       "3   Sarah Boseley      1  https://www.theguardian.com/science/2020/may/2...   \n",
       "4       Ed Aarons      1  https://www.theguardian.com/sport/2020/mar/13/...   \n",
       "\n",
       "             published                                               text  \\\n",
       "0  2020-05-25T00:00:00  Key developments in the global coronavirus out...   \n",
       "1  2020-05-24T00:00:00  Explainer: what do we now know about Covid-19 ...   \n",
       "2  2020-05-22T00:00:00  Politicians have become more cautious about im...   \n",
       "3  2020-05-22T00:00:00  Hydroxychloroquine, the anti-malarial drug Don...   \n",
       "4  2020-03-13T00:00:00  From major club and international football to ...   \n",
       "\n",
       "                                               title  \n",
       "0                           Coronavirus: at a glance  \n",
       "1  Explainer: what do we now know about Covid-19 ...  \n",
       "2         Why we might not get a coronavirus vaccine  \n",
       "3  Hydroxychloroquine: Trump's Covid-19 'cure' in...  \n",
       "4  Coronavirus and sport – a list of the major ca...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Cleaning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    266\n",
       "0    266\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "      <th>link</th>\n",
       "      <th>published</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Helen Sullivan</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.theguardian.com/world/2020/may/25/...</td>\n",
       "      <td>2020-05-25T00:00:00</td>\n",
       "      <td>Key developments in the global coronavirus out...</td>\n",
       "      <td>Coronavirus: at a glance</td>\n",
       "      <td>Key developments in the global coronavirus out...</td>\n",
       "      <td>[Key, developments, in, the, global, coronavir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nicola Davis</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.theguardian.com/world/2020/may/24/...</td>\n",
       "      <td>2020-05-24T00:00:00</td>\n",
       "      <td>Explainer: what do we now know about Covid-19 ...</td>\n",
       "      <td>Explainer: what do we now know about Covid-19 ...</td>\n",
       "      <td>Explainer: what do we now know about Covid-19 ...</td>\n",
       "      <td>[Explainer, what, do, we, now, know, about, Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ian Sample</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.theguardian.com/world/2020/may/22/...</td>\n",
       "      <td>2020-05-22T00:00:00</td>\n",
       "      <td>Politicians have become more cautious about im...</td>\n",
       "      <td>Why we might not get a coronavirus vaccine</td>\n",
       "      <td>Politicians have become more cautious about im...</td>\n",
       "      <td>[Politicians, have, become, more, cautious, ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sarah Boseley</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.theguardian.com/science/2020/may/2...</td>\n",
       "      <td>2020-05-22T00:00:00</td>\n",
       "      <td>Hydroxychloroquine, the anti-malarial drug Don...</td>\n",
       "      <td>Hydroxychloroquine: Trump's Covid-19 'cure' in...</td>\n",
       "      <td>Hydroxychloroquine, the anti-malarial drug Don...</td>\n",
       "      <td>[Hydroxychloroquine, the, anti, malarial, drug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ed Aarons</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.theguardian.com/sport/2020/mar/13/...</td>\n",
       "      <td>2020-03-13T00:00:00</td>\n",
       "      <td>From major club and international football to ...</td>\n",
       "      <td>Coronavirus and sport – a list of the major ca...</td>\n",
       "      <td>From major club and international football to ...</td>\n",
       "      <td>[From, major, club, and, international, footba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author  label                                               link  \\\n",
       "0  Helen Sullivan      1  https://www.theguardian.com/world/2020/may/25/...   \n",
       "1    Nicola Davis      1  https://www.theguardian.com/world/2020/may/24/...   \n",
       "2      Ian Sample      1  https://www.theguardian.com/world/2020/may/22/...   \n",
       "3   Sarah Boseley      1  https://www.theguardian.com/science/2020/may/2...   \n",
       "4       Ed Aarons      1  https://www.theguardian.com/sport/2020/mar/13/...   \n",
       "\n",
       "             published                                               text  \\\n",
       "0  2020-05-25T00:00:00  Key developments in the global coronavirus out...   \n",
       "1  2020-05-24T00:00:00  Explainer: what do we now know about Covid-19 ...   \n",
       "2  2020-05-22T00:00:00  Politicians have become more cautious about im...   \n",
       "3  2020-05-22T00:00:00  Hydroxychloroquine, the anti-malarial drug Don...   \n",
       "4  2020-03-13T00:00:00  From major club and international football to ...   \n",
       "\n",
       "                                               title  \\\n",
       "0                           Coronavirus: at a glance   \n",
       "1  Explainer: what do we now know about Covid-19 ...   \n",
       "2         Why we might not get a coronavirus vaccine   \n",
       "3  Hydroxychloroquine: Trump's Covid-19 'cure' in...   \n",
       "4  Coronavirus and sport – a list of the major ca...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  Key developments in the global coronavirus out...   \n",
       "1  Explainer: what do we now know about Covid-19 ...   \n",
       "2  Politicians have become more cautious about im...   \n",
       "3  Hydroxychloroquine, the anti-malarial drug Don...   \n",
       "4  From major club and international football to ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Key, developments, in, the, global, coronavir...  \n",
       "1  [Explainer, what, do, we, now, know, about, Co...  \n",
       "2  [Politicians, have, become, more, cautious, ab...  \n",
       "3  [Hydroxychloroquine, the, anti, malarial, drug...  \n",
       "4  [From, major, club, and, international, footba...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = X\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_clean['clean'] = df_clean['text'].astype('str') \n",
    "df_clean.dtypes\n",
    "\n",
    "df_clean[\"tokens\"] = df_clean[\"clean\"].apply(tokenizer.tokenize)\n",
    "# delete Stop Words\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_clean['clean']\n",
    "\n",
    "Y = df_clean['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, Y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Turning the texts into vectors, TFIDF, and N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(token_pattern = r'\\w{1,}')\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "count_vect.fit(df_clean['clean'])\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(2, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word',\n",
    "                                   token_pattern = r'\\w{1,}',\n",
    "                                   ngram_range = (2, 3),\n",
    "                                   max_features = 5000)\n",
    "print(tfidf_vect_ngram)\n",
    "\n",
    "tfidf_vect_ngram.fit(df_clean['clean'])\n",
    "X_train_tfidf_ngram = tfidf_vect_ngram.transform(X_train)\n",
    "X_test_tfidf_ngram  = tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
      "        ngram_range=(2, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = 'char',\n",
    "                                         token_pattern = r'\\w{1,}',\n",
    "                                         ngram_range = (2, 3),\n",
    "                                         max_features = 5000)\n",
    "print(tfidf_vect_ngram_chars)\n",
    "\n",
    "tfidf_vect_ngram_chars.fit(df_clean['clean'])\n",
    "X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_train)\n",
    "X_test_tfidf_ngram_chars  = tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 1. Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_runner(classifier, X_train, X_test):\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    train_accuracy = accuracy_score(Y_train, classifier.predict(X_train))\n",
    "    test_accuracy = accuracy_score(Y_test, classifier.predict(X_test))\n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Setting default iteration to 350."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1 = LogisticRegression(solver = 'lbfgs', max_iter = 350, random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier1, X_train_count, X_test_count)\n",
    "\n",
    "row_list.append(['1', 'Base', 'Logistic Regression', 'Count_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = LogisticRegression(solver = 'lbfgs', max_iter = 350, random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier2, X_train_tfidf_ngram, X_test_tfidf_ngram)\n",
    "\n",
    "row_list.append(['2', 'Base', 'Logistic Regression', 'TFIDF', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = LogisticRegression(solver = 'lbfgs', max_iter = 350, random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier3, X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars)\n",
    "\n",
    "row_list.append(['3', 'Base', 'Logistic Regression', 'N_grams', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier4 = LinearSVC(random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier4, X_train_count, X_test_count)\n",
    "\n",
    "row_list.append(['4', 'Base', 'Linear SVC', 'Count_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier5 = LinearSVC(random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier5, X_train_tfidf_ngram, X_test_tfidf_ngram)\n",
    "\n",
    "row_list.append(['5', 'Base', 'Linear SVC', 'TFIDF', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier6 = LinearSVC(random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier6, X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars)\n",
    "\n",
    "row_list.append(['6', 'Base', 'Linear SVC', 'N_grams', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier7 = KNeighborsClassifier()\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier7, X_train_count, X_test_count)\n",
    "\n",
    "row_list.append(['7', 'Base', 'K-Neighbors', 'Count_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier8 = KNeighborsClassifier()\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier8, X_train_tfidf_ngram, X_test_tfidf_ngram)\n",
    "\n",
    "row_list.append(['8', 'Base', 'K-Neighbors', 'TFIDF', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier9 = KNeighborsClassifier()\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier9, X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars)\n",
    "\n",
    "row_list.append(['9', 'Base', 'K-Neighbors', 'N_grams', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier10 = DecisionTreeClassifier(random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier10, X_train_count, X_test_count)\n",
    "\n",
    "row_list.append(['10', 'Base', 'Decision Tree', 'Count_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier11 = DecisionTreeClassifier(random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier11, X_train_tfidf_ngram, X_test_tfidf_ngram)\n",
    "\n",
    "row_list.append(['11', 'Base', 'Decision Tree', 'TFIDF', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier12 = DecisionTreeClassifier(random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier12, X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars)\n",
    "\n",
    "row_list.append(['12', 'Base', 'Decision Tree', 'N_grams', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Parameter tuned\n",
    "\n",
    "Setting assumption\n",
    "- N estimator = 500\n",
    "    - This doesn't affect the movement of accuracy\n",
    "- Max Depth = 3\n",
    "- Sample Split = 40\n",
    "- Sample Leaf = 44\n",
    "- Leaf Nodes = 30\n",
    "- Weight Fraction = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier13 = RandomForestClassifier(n_estimators = 500, \\\n",
    "                                      max_depth = 3,\\\n",
    "                                      min_samples_split = 40, \\\n",
    "                                      min_samples_leaf = 44, \\\n",
    "                                      max_leaf_nodes = 30, \\\n",
    "                                      min_weight_fraction_leaf = 0.2, \\\n",
    "                                      random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier13, X_train_count, X_test_count)\n",
    "\n",
    "row_list.append(['13', 'Random Forest', 'Random Forest Classifier', 'Count_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier14 = RandomForestClassifier(n_estimators = 500, \\\n",
    "                                      max_depth = 3,\\\n",
    "                                      min_samples_split = 40, \\\n",
    "                                      min_samples_leaf = 44, \\\n",
    "                                      max_leaf_nodes = 30, \\\n",
    "                                      min_weight_fraction_leaf = 0.2, \\\n",
    "                                      random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier14, X_train_tfidf_ngram, X_test_tfidf_ngram)\n",
    "\n",
    "row_list.append(['14', 'Random Forest', 'Random Forest Classifier', 'TFIDF', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier15 = RandomForestClassifier(n_estimators = 500, \\\n",
    "                                      max_depth = 3,\\\n",
    "                                      min_samples_split = 40, \\\n",
    "                                      min_samples_leaf = 44, \\\n",
    "                                      max_leaf_nodes = 30, \\\n",
    "                                      min_weight_fraction_leaf = 0.2, \\\n",
    "                                      random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier15, X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars)\n",
    "\n",
    "row_list.append(['15', 'Random Forest', 'Random Forest Classifier', 'N_grams', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier\n",
    "\n",
    "Using Logistic regression, KNN & Naive Bayes for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier16 = BaggingClassifier(LogisticRegression(solver = 'lbfgs', max_iter = 350, random_state = 0), \\\n",
    "                                 n_estimators = 500, \\\n",
    "                                 max_samples = 44, \\\n",
    "                                 max_features = 40, \\\n",
    "                                 random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier16, X_train_count, X_test_count)\n",
    "\n",
    "row_list.append(['16', 'Bagging', 'Logistic Regression', 'Count_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier17 = BaggingClassifier(LogisticRegression(solver = 'lbfgs', max_iter = 350, random_state = 0), \\\n",
    "                                 n_estimators = 500, \\\n",
    "                                 max_samples = 44, \\\n",
    "                                 max_features = 40, \\\n",
    "                                 random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier17, X_train_tfidf_ngram, X_test_tfidf_ngram)\n",
    "\n",
    "row_list.append(['17', 'Bagging', 'Logistic Regression', 'TFIDF', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier18 = BaggingClassifier(LogisticRegression(solver = 'lbfgs', max_iter = 350, random_state = 0), \\\n",
    "                                 n_estimators = 500, \\\n",
    "                                 max_samples = 44, \\\n",
    "                                 max_features = 40, \\\n",
    "                                 random_state = 0)\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier18, X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars)\n",
    "\n",
    "row_list.append(['18', 'Bagging', 'Logistic Regression', 'N_grams', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier19 = BaggingClassifier(KNeighborsClassifier())\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier19, X_train_count, X_test_count)\n",
    "\n",
    "row_list.append(['19', 'Bagging', 'K-Neighbors', 'Count_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier20 = BaggingClassifier(KNeighborsClassifier())\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier20, X_train_tfidf_ngram, X_test_tfidf_ngram)\n",
    "\n",
    "row_list.append(['20', 'Bagging', 'K-Neighbors', 'TFIDF', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier21 = BaggingClassifier(KNeighborsClassifier())\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier21, X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars)\n",
    "\n",
    "row_list.append(['21', 'Bagging', 'K-Neighbors', 'N_grams', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier25 = BaggingClassifier(MultinomialNB())\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier25, X_train_count, X_test_count)\n",
    "\n",
    "row_list.append(['25', 'Bagging', 'Naive Bayes', 'Count_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier26 = BaggingClassifier(MultinomialNB())\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier26, X_train_tfidf_ngram, X_test_tfidf_ngram)\n",
    "\n",
    "row_list.append(['26', 'Bagging', 'Naive Bayes', 'TFIDF', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier27 = BaggingClassifier(MultinomialNB())\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier27, X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars)\n",
    "\n",
    "row_list.append(['27', 'Bagging', 'Naive Bayes', 'N_grams', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier22 = MultinomialNB()\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier22, X_train_count, X_test_count)\n",
    "\n",
    "row_list.append(['22', 'Base', 'Naive Bayes', 'Count_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier23 = MultinomialNB()\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier23, X_train_tfidf_ngram, X_test_tfidf_ngram)\n",
    "\n",
    "row_list.append(['23', 'Base', 'Naive Bayes', 'TFIDF', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier24 = MultinomialNB()\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier24, X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars)\n",
    "\n",
    "row_list.append(['24', 'Base', 'Naive Bayes', 'N_grams', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier28 = Perceptron()\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier28, X_train_count, X_test_count)\n",
    "\n",
    "row_list.append(['28', 'Base', 'Perceptron', 'Count_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier29 = Perceptron()\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier29, X_train_tfidf_ngram, X_test_tfidf_ngram)\n",
    "\n",
    "row_list.append(['29', 'Base', 'Perceptron', 'TFIDF', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier30 = Perceptron()\n",
    "\n",
    "train_accuracy, test_accuracy = classifier_runner(classifier30, X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars)\n",
    "\n",
    "row_list.append(['30', 'Base', 'Perceptron', 'N_grams', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Neural Network - Attempt\n",
    "\n",
    "I borrowed the codes to build the neural network as I'm still an amateur at it. \n",
    "\n",
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = X\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "t = time()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_clean['clean'] = df_clean['text'].astype('str') \n",
    "df_clean.dtypes\n",
    "\n",
    "df_clean[\"tokens\"] = df_clean[\"clean\"].apply(tokenizer.tokenize)\n",
    "# delete Stop Words\n",
    "\n",
    "print('Time to tokenize everything: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "\n",
    "t = time()\n",
    "w2v_model.build_vocab(df_clean[\"tokens\"], progress_per=1000)\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "w2v_model.train(df_clean[\"tokens\"], total_examples=w2v_model.corpus_count, epochs=10000, report_delay=1)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['label'].values\n",
    "x = np.array(X[\"clean\"])\n",
    "\n",
    "#And here is the train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2v_model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_train)])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_test)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "print ('shape for training set : ',train_vecs_w2v.shape,\n",
    "      '\\nshape for test set : ', test_vecs_w2v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Dense(128, activation='relu', input_dim=300))\n",
    "model1.add(Dropout(0.7))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model1.fit(train_vecs_w2v, y_train, epochs = 300, batch_size = 50, validation_data = (test_vecs_w2v, y_test))\n",
    "loss, train_accuracy = model1.evaluate(train_vecs_w2v, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "loss, test_accuracy = model1.evaluate(test_vecs_w2v, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "row_list.append(['31', 'Neural Network', 'ReLU', 'Word_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(128, activation='softmax', input_dim = 300))\n",
    "model2.add(Dropout(0.7))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model2.fit(train_vecs_w2v, y_train, epochs = 300, batch_size=50, validation_data=(test_vecs_w2v, y_test))\n",
    "loss, train_accuracy = model2.evaluate(train_vecs_w2v, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "loss, test_accuracy = model2.evaluate(test_vecs_w2v, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "row_list.append(['32', 'Neural Network', 'SoftMax', 'Word_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Dense(128, activation='tanh', input_dim=300))\n",
    "model3.add(Dropout(0.7))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model3.fit(train_vecs_w2v, y_train, epochs = 300, batch_size = 50, validation_data = (test_vecs_w2v, y_test))\n",
    "loss, train_accuracy = model3.evaluate(train_vecs_w2v, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "loss, test_accuracy = model3.evaluate(test_vecs_w2v, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "row_list.append(['33', 'Neural Network', 'TanH', 'Word_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "## Model 2 - Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for tokens in X for word in tokens]\n",
    "all_sentence_lengths = [len(tokens) for tokens in X]\n",
    "ALL_VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(ALL_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(all_sentence_lengths))\n",
    "\n",
    "\n",
    "####################### CHANGE THE PARAMETERS HERE #####################################\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 30# how many unique words to use\n",
    "MAX_SEQUENCE_LENGTH = 9 # max number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE, lower = True, char_level = False)\n",
    "tokenizer.fit_on_texts(X[\"text\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train.tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index) + 1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = w2v_model[word] if word in w2v_model else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "\n",
    "######################## TRAIN AND TEST SET #################################\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable = False, extra_conv = True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights = [embeddings],\n",
    "                            input_length = max_sequence_length,\n",
    "                            trainable = trainable)\n",
    "\n",
    "    sequence_input = Input(shape = (max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters = 128, kernel_size = filter_size, activation = 'relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size = 3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate([convs[0],convs[1],convs[2]],axis=1)\n",
    "    \n",
    "    conv = Conv1D(filters = 128, kernel_size = 3, activation = 'sigmoid')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size = 3)(conv)\n",
    "    \n",
    "    if extra_conv == True:\n",
    "        x = Dropout(0.5)(l_merge)\n",
    "    else:\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation = 'sigmoid')(x)\n",
    "    preds = Dense(1, activation='relu')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adadelta', metrics = ['acc'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index) + 1, EMBEDDING_DIM, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_cnn_data, y_train, epochs = 10, batch_size = 50, validation_data=(test_cnn_data, y_test))\n",
    "\n",
    "loss, train_accuracy = model.evaluate(train_cnn_data, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "loss, test_accuracy = model.evaluate(test_cnn_data, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "row_list.append(['34', 'Neural Network', 'Multi-layer', 'Word_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.initializers import normal\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, initializer, trainable = False, extra_conv = True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights = [embeddings],\n",
    "                            input_length = max_sequence_length,\n",
    "                            trainable = trainable)\n",
    "\n",
    "    sequence_input = Input(shape = (max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters = 128, kernel_size = filter_size, activation = 'sigmoid')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size = 3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate([convs[0],convs[1],convs[2]],axis=1)\n",
    "    \n",
    "    conv = Conv1D(filters = 128, kernel_size = 3, activation = 'sigmoid')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size = 3)(conv)\n",
    "    \n",
    "    if extra_conv == True:\n",
    "        x = Dropout(0.5)(l_merge)\n",
    "    else:\n",
    "        x = Dropout(0.5)(pool)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation = 'sigmoid', kernel_initializer = initializer)(x)\n",
    "    preds = Dense(1, activation = 'tanh')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adadelta', metrics = ['acc'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index) + 1, EMBEDDING_DIM, 'he_normal', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_cnn_data, y_train, epochs = 20, batch_size = 50, validation_data = (test_cnn_data, y_test))\n",
    "\n",
    "loss, train_accuracy = model.evaluate(train_cnn_data, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "loss, test_accuracy = model.evaluate(test_cnn_data, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "row_list.append(['34', 'Neural Network', 'Multi-layer', 'Word_Vector', train_accuracy, test_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dataframe = pd.DataFrame(row_list, columns = ['Classifier No.', 'Type', 'Classifier', 'Method', 'train_accuracy', 'test_accuracy'])\n",
    "accuracy_dataframe.set_index('Classifier No.', inplace = True)\n",
    "accuracy_dataframe.drop_duplicates(inplace = True)\n",
    "accuracy_dataframe.sort_values('test_accuracy', axis = 0, ascending = False, inplace = True)\n",
    "\n",
    "accuracy_dataframe['Robust'] = (accuracy_dataframe['train_accuracy'] - accuracy_dataframe['test_accuracy'])/accuracy_dataframe['train_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dataframe[accuracy_dataframe['Robust'] <= 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the url only here\n",
    "\n",
    "url_input = \"https://www.wsj.com/articles/germany-sees-largest-local-covid-19-outbreak-since-lifting-lockdown-11592415003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles downloaded from newspaper url:  https://www.wsj.com/articles/germany-sees-largest-local-covid-19-outbreak-since-lifting-lockdown-11592415003\n"
     ]
    }
   ],
   "source": [
    "paper = Article(url_input)\n",
    "newsPaper = {}\n",
    "\n",
    "paper.download()\n",
    "paper.parse()\n",
    "newsPaper['title'] = paper.title\n",
    "newsPaper['text'] = paper.text\n",
    "newsPaper['link'] = paper.url\n",
    "newsPaper['author'] = paper.authors\n",
    "print(\"articles downloaded from newspaper url: \", paper.url)\n",
    "\n",
    "X_input = pd.DataFrame(newsPaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany Sees Largest Local Covid-19 Outbreak S...</td>\n",
       "      <td>BERLIN—Germany has recorded its largest local ...</td>\n",
       "      <td>https://www.wsj.com/articles/germany-sees-larg...</td>\n",
       "      <td>Bojan Pancevski</td>\n",
       "      <td>BERLIN—Germany has recorded its largest local ...</td>\n",
       "      <td>[BERLIN, Germany, has, recorded, its, largest,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Germany Sees Largest Local Covid-19 Outbreak S...   \n",
       "\n",
       "                                                text  \\\n",
       "0  BERLIN—Germany has recorded its largest local ...   \n",
       "\n",
       "                                                link           author  \\\n",
       "0  https://www.wsj.com/articles/germany-sees-larg...  Bojan Pancevski   \n",
       "\n",
       "                                               clean  \\\n",
       "0  BERLIN—Germany has recorded its largest local ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [BERLIN, Germany, has, recorded, its, largest,...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = X_input\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_clean['clean'] = df_clean['text'].astype('str') \n",
    "df_clean.dtypes\n",
    "\n",
    "df_clean[\"tokens\"] = df_clean[\"clean\"].apply(tokenizer.tokenize)\n",
    "# delete Stop Words\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = df_clean['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = classifier24.predict(X_predict_tfidf_ngram_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = classifier24.predict_proba(X_predict_tfidf_ngram_chars)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_clean[['link', 'author', 'title', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['predict'] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['probability'] = probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>predict</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.wsj.com/articles/germany-sees-larg...</td>\n",
       "      <td>Bojan Pancevski</td>\n",
       "      <td>Germany Sees Largest Local Covid-19 Outbreak S...</td>\n",
       "      <td>BERLIN—Germany has recorded its largest local ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link           author  \\\n",
       "0  https://www.wsj.com/articles/germany-sees-larg...  Bojan Pancevski   \n",
       "\n",
       "                                               title  \\\n",
       "0  Germany Sees Largest Local Covid-19 Outbreak S...   \n",
       "\n",
       "                                                text  predict  probability  \n",
       "0  BERLIN—Germany has recorded its largest local ...        1     0.532927  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to add output export codes after this line here if needed.\n",
    "\n",
    "-------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
